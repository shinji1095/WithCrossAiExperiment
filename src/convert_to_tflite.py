import os
from pathlib import Path
import torch
import torch.nn as nn
import timm
import tensorflow as tf
from models.model import get_model


@torch.no_grad()
def _get_in_features(backbone: nn.Module) -> int:
    """
    ãƒ€ãƒŸãƒ¼å…¥åŠ›ã§æœ€çµ‚ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«é•·ã‚’ç›´æ¥æ¸¬å®šã€‚
    ã©ã® timm ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã§ã‚‚æ­£ç¢ºã€‚
    """
    backbone.eval()                     # â† å¿µã®ãŸã‚
    cfg = backbone.default_cfg
    c, h, w = cfg.get("input_size", (3, 224, 224))
    dummy   = torch.zeros(1, c, h, w, device=next(backbone.parameters()).device)
    feat    = backbone(dummy)
    return feat.shape[1]


class ClassificationModel(nn.Module):
    def __init__(self, backbone_name: str, num_classes: int):
        super().__init__()
        self.backbone = timm.create_model(backbone_name,
                                          pretrained=True,
                                          num_classes=0)  # ãƒ˜ãƒƒãƒ‰ãªã—
        in_features   = _get_in_features(self.backbone)   # â† æ–°ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
        self.head     = nn.Linear(in_features, num_classes)

    def forward(self, x):
        feat = self.backbone(x)        # shape: (B, in_features)
        return self.head(feat)

# =============================================
# Colab: timm â†’ ONNX â†’ Keras/SavedModel â†’ TFLite
# ãƒ¢ãƒ‡ãƒ«ã”ã¨ã«å…¥åŠ›ã‚µã‚¤ã‚ºã‚’æŒ‡å®š
# (c) 2025
# =============================================

# --- ãƒ¢ãƒ‡ãƒ«ã¨å…¥åŠ›ã‚µã‚¤ã‚ºã®å¯¾å¿œè¡¨ï¼ˆå¿…è¦ã«å¿œã˜ã¦è¿½åŠ ï¼‰ ---
MODELS = {
    # 'convnext_tiny.in12k_ft_in1k_best': 288,
    # 'convnext_nano.in12k_ft_in1k_best': 288,
    # 'hgnetv2_b4.ssld_stage2_ft_in1k_best': 288,
    # 'vit_medium_patch16_gap_256.sw_in12k_ft_in1k_best': 256,
    'edgenext_base.usi_in1k': 320,
    # 'hgnetv2_b3.ssld_stage2_ft_in1k_best': 288,
    # 'edgenext_base.in21k_ft_in1k_best': 320,
    # 'vit_mediumd_patch16_reg4_gap_256.sbb2_e200_in12k_ft_in1k_best': 256,
    # 'tf_efficientnet_b4.ap_in1k_best': 380,
}

WEIGHTS_DIR = Path('weight/pytorch')
WEIGHTS_DIR.mkdir(exist_ok=True, parents=True)

# --- å¤±æ•—è¨˜éŒ²ç”¨ãƒªã‚¹ãƒˆ ---
errors = []

# --- å¤‰æ›ãƒ«ãƒ¼ãƒ— ---
for model_name, image_size in MODELS.items():
    print(f'\n==================== {model_name} å¤‰æ›é–‹å§‹ ====================')
    try:
        ROOT_DIR = Path(f'weight')
        ROOT_DIR.mkdir(exist_ok=True, parents=True)

        ONNX_MODEL_PATH = ROOT_DIR/ 'onnx' / f'{model_name}.onnx'
        ONNX2TF_OUTPUT_DIR = ROOT_DIR / 'tflite' / model_name
        TFLITE_MODEL_PATH = ROOT_DIR / f'{model_name}.tflite'

        # --- 1ï¸âƒ£ timm â†’ ONNX ---
        # model = ClassificationModel(model_name.replace("_best", ""), 3).eval()
        model = get_model(
            'classification',
            model_name,
            num_classes=3,
            dropout_rate=0.0,
            drop_path_rate=0.0,
        ).eval()

        # --- é‡ã¿ãƒ­ãƒ¼ãƒ‰ ---
        weights_path = WEIGHTS_DIR / f'{model_name}.pth'
        if weights_path.exists():
            state_dict = torch.load(weights_path, map_location='cpu')
            model.load_state_dict(state_dict)
            print(f'[âœ“] é‡ã¿ãƒ­ãƒ¼ãƒ‰å®Œäº†: {weights_path}')
        else:
            raise FileNotFoundError(f'é‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {weights_path}')

        dummy_input = torch.randn(1, 3, image_size, image_size)
        torch.onnx.export(
            model,
            dummy_input,
            str(ONNX_MODEL_PATH),
            input_names=['input'],
            output_names=['output'],
            opset_version=14
        )
        print(f'[âœ“] ONNXã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†: {ONNX_MODEL_PATH}')


        # --- 2ï¸âƒ£ ONNX â†’ Keras/SavedModel ---
        result = os.system(
            f'onnx2tf -i {ONNX_MODEL_PATH} '
            f'-osd -oh5 -b 1 -kt /input '
            f'-ois 1,3,{image_size},{image_size} '
            f'-o {ONNX2TF_OUTPUT_DIR}'
        )
        if result != 0:
            raise RuntimeError('onnx2tfã‚³ãƒãƒ³ãƒ‰ã§å¤±æ•—')

        print(f'[âœ“] Keras/SavedModelç”Ÿæˆå®Œäº†: {ONNX2TF_OUTPUT_DIR}')

        # --- 3ï¸âƒ£ SavedModel â†’ TFLite ---
        converter = tf.lite.TFLiteConverter.from_saved_model(str(ONNX2TF_OUTPUT_DIR))

        # å®‰å®šåŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        converter.experimental_new_converter = True
        converter._experimental_lower_tensor_list_ops = True

        # Flex ã‚’è¨±å¯ï¼ˆå†…è”µOPã§è¶³ã‚Šãªã„åˆ†ã ã‘ Flex ã«å›ã—ã€å¤§åŠã¯ XNNPACK ã«ä¹—ã›ã‚‹ï¼‰
        converter.target_spec.supported_ops = [
            tf.lite.OpsSet.TFLITE_BUILTINS,
            tf.lite.OpsSet.SELECT_TF_OPS,
        ]

        # ï¼ˆä»»æ„ï¼‰ã‚µã‚¤ã‚º/å¸¯åŸŸã‚’ä¸‹ã’ãŸã„å ´åˆã¯ â€œé‡ã¿ã ã‘â€ FP16ã€‚å…¥å‡ºåŠ›ã¯ float32 ã®ã¾ã¾ã€‚
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        # converter.target_spec.supported_types = [tf.float16]

        tflite_model = converter.convert()
        TFLITE_MODEL_PATH.write_bytes(tflite_model)

        print(f'[âœ“] TFLiteå¤‰æ›å®Œäº†: {TFLITE_MODEL_PATH}')

        
    except Exception as e:
        print(f'[âœ—] {model_name} å¤‰æ›å¤±æ•—: {e}')
        errors.append({'model': model_name, 'error': str(e)})

# --- çµæœå ±å‘Š ---
if errors:
    print('\n==================== å¤‰æ›å¤±æ•—ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ ====================')
    for entry in errors:
        print(f"ãƒ»{entry['model']}: {entry['error']}")
else:
    print('\nğŸ‰ å…¨ãƒ¢ãƒ‡ãƒ«æ­£å¸¸å¤‰æ›å®Œäº† ğŸ‰')
